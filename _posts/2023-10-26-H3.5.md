---
title: Statistic H_3.5
date: 2023-10-26 18:18:11 +/-0000
categories: [learning]
math: true
---


# Practical part 

Revise and optimize you previous programs to compute the joint distribution of any number of 2,3, ...k, continuous quantitative variables
where, for each variable, the user can specify the number of subdivisions ("class intervals") of a range containing the observed values.

Revise also your previous homework taking into account that qualitative variables can be ordered and therefore the order needs to be preserved.

For quantitative variables, include the possibility to specify class intervals too.

# Implementation

The prevous version of the program was integrated to support the joint of K continuous variables, 

{% include embed/youtube.html id='DPkYLyuy6Us' %}
                                  
The text field enable the user to choose how many intervals he wants for each variable, the first number refers to the first (not temporarly) selected variable in the checkbox below

![Desktop View](/assets/statiistics/H3.5/n1.png){: w="350"}

In the code we have few key functions, the first **"dividi_classi"** will calculate for each variables the different K intervals and add them to a List<(float,float):
```c#

 List<(float, float)> tuple_intervalli = new List<(float, float)>();



 List<float> l = new List<float>();

 l = Diz_tot[nome].Select(float.Parse).ToList();                        // The list used come from the initial parse of the input file

 float max = l.Max();
 
 float min = l.Min();

 float smallest = l.Max();
 float secondSmallest=l.Max();

 if (nome == "height" || nome == "weight")                   // the value 0 cannot be consider a valid minimum for these two variables so i need to calculate the net smallest value
 {
     foreach (float number in l)
     {
         if (number < smallest)
         {
             secondSmallest = smallest;
             smallest = number;
         }
         else if (number < secondSmallest && number != smallest)
         {
             secondSmallest = number;
         }
     }
     min = secondSmallest;
 }


 float dim = (max - min) / (float)intervalli;                          // calcolo grandezza ogni intervallo




 for (int i = 0; i < intervalli; i++)                   // alcolo ogni intervallo
 {
     float s = min + i * dim;
     float s1 = s + dim;
     intervals_quanti.Add((s, s1), 0);

     tuple_intervalli.Add((s, s1));                          // aggiungo l'intervallo alla lista degli intervalli

     

 }

```
The second one **"check_interval"** will check each row and associate for each of the attribute of interest which is the associated interval 
```c#

float valore1 = float.Parse(valore);          // il valore arriva come stringa

string init="";
string fin="";
string s = "";



foreach (var interval in totale_var_int[index])       
{
    int intervallo = totale_var_int[index].Count-1;
    if (valore1 == totale_var_int[index][intervallo].Item2)                 // check specifico se il valore è l'ultimo nell'ultmo intervallo (altrimenti andrebbe perso)
    {
        init = totale_var_int[index][intervallo].Item1.ToString();
        fin = totale_var_int[index][intervallo].Item2.ToString();

        s = "("+init + ";" + fin + ")";
        return s;

    }

    if (valore1 >= interval.Item1 && valore1 < interval.Item2)                      //  controllo a quale intervallo il valore appariene e 
    {
         s = "("+interval.Item1.ToString() + ";" + interval.Item2.ToString()+")";

        init = interval.Item1.ToString();
        fin = interval.Item2.ToString();
        return s;
    }
    
}

s = init + "," + fin;

return s;
```
After finding all the related k intervals a third function will add a +1 to a key in a dictionary build like this:  "key=interval_variable_1,interval_variable_2,.. value=how_many" (and repeat the process for each row)


## The updated code can be found in the following [Repo](https://github.com/Cheroberous/Statistic/tree/main/H3.5/)



## Qualitatives variables

Ordering qualitative variables involves arranging the categories in a meaningful order. The order must be explicitly requested, can be based on natural relationships or the specific context of the data analysis.
Several options can be considered:  <br>

+ Natural Order: <br>

If the categories have a natural order, you can order them accordingly. For example, for the variable "Education Level," you can order the categories from "No Education" to "Elementary," "High School," "College," "Bachelor's," "Master's," "Ph.D.," etc. This order reflects the natural progression of education.

+ Ordinal Variables: <br>

If the qualitative variable represents an ordinal scale (i.e., the categories have an inherent order), you should order them according to this scale. For instance, for "Satisfaction Level," you can order the categories as "Very Dissatisfied," "Dissatisfied," "Neutral," "Satisfied," and "Very Satisfied."

+ Frequency or Prevalence: <br>

You can order the categories based on their frequency or prevalence in your dataset. More frequent categories can be listed first, providing a sense of the most common occurrences.
Expert Opinion:

In some cases, domain experts or researchers may provide guidance on the order of qualitative variables based on their expertise or the specific goals of the analysis. Expert opinions can be valuable in determining an appropriate order.

+ ...

A preference have to be expressed in order to implement an algorithm that that it into account.





# Theory 

Research

(Revise and improve your simulations in homework 3, where necessary.)
Search on the web about the Law of large numbers LLN and compare it with Part b of your homework 3 and express in your own words whether your simulation is somehow related with this theorem, and why.
Search on the web about the Central Limit Theorem CLT and compare it with Part a of your homework 3 and say in your own words whether your simulation is somehow related with this theorem, and why.
Based on the CLT, how could you modify ("normalize") the "security score" to obtain an asymptotic convergence to a proper distribution?

## LLN 
The law of large numbers states that even random events with a large number of trials may return stable long-term results. Note that the theorem deals only with a large number of trials while the average of the results of the experiment repeated a small number of times might be substantially different from the expected value. <br>

A Law of Large Numbers (LLN) is a proposition that provides a set of sufficient conditions for the convergence of the sample mean to a constant.

Let {Xn} be a sequence of random variables.

Let Xn be the sample mean of the first n terms of the sequence:

$$
  \overline{X_n} =  \frac{1}{n}\sum_{i=1}^n X_i  
$$
<br>
A Law of Large Numbers (LLN) states some conditions that are sufficient to guarantee the convergence of Xn to a constant, as the sample size n increases.

Typically, the constant is the expected value of the distribution from which the sample has been drawn:

![g1](/assets/statiistics/H3.5/G1.gif)
_demo_

Typically, all the random variables in the sequence { $X_n$ } have the same expected value  $E\left[ X_n \right] = \mu$ . In this case, the constant to which the sample mean converges is mu (which is called population mean).

#Example of Law of Large Numbers

The simplest example of the law of large numbers is rolling the dice. The dice involves six different events with equal probabilities. The expected value of the dice events is:

$$
EV=\frac{1+2+3+4+5+6}{6}=3.5
$$

 

If we roll the dice only three times, the average of the obtained results may be far from the expected value. Let’s say you rolled the dice three times and the outcomes were 6, 6, 3. The average of the results is 5. According to the law of the large numbers, if we roll the dice a large number of times, the average result will be closer to the expected value of 3.5.

## COMPARE WITH PART B 

The part B of the previous homework asked to "simulate the cumulated frequency, f, of penetration. There the probability of fail/success was fixed and chosen by the user so given the LLN should converge towards the expected penetration probability (p) for a single attack. This means that with a large number of attacks, the observed cumulative frequency should approach the true probability of penetration.

## CLT

The Central Limit Theorem (CLT) is a statistical concept that states that the sample mean distribution of a random variable will assume a near-normal or normal distribution if the sample size is large enough. In simple terms, the theorem states that the sampling distribution of the mean approaches a normal distribution as the size of the sample increases, regardless of the shape of the original population distribution. <br>


![g2](/assets/statiistics/h3/normal.png)
_demo_

As the user increases the number of samples to 30, 40, 50, etc., the graph of the sample means will move towards a normal distribution. The sample size must be 30 or higher for the central limit theorem to hold.

One of the most important components of the theorem is that the **mean of the sample will be the mean of the entire population**. If you calculate the mean of multiple samples of the population, add them up, and find their average, the result will be the estimate of the population mean. <br>

# How Does the Central Limit Theorem Work? 
The central limit theorem forms the basis of the probability distribution. It makes it easy to understand how population estimates behave when subjected to repeated sampling. When plotted on a graph, the theorem shows the shape of the distribution formed by means of repeated population samples.

As the sample sizes get bigger, the distribution of the means from the repeated samples tends to normalize and resemble a normal distribution. The result remains the same regardless of what the original shape of the distribution was. It can be illustrated in the figure below:

![g2](/assets/statiistics/H3.5/bell.png)
_demo_

From the figure above, we can deduce that despite the fact that the original shape of the distribution was uniform, it tends towards a normal distribution as the value of n (sample size) increases.
Apart from showing the shape that the sample means will take, the central limit theorem also gives an overview of the mean and variance of the distribution. The sample mean of the distribution is the actual population mean from which the samples were taken.
The variance of the sample distribution, on the other hand, is the variance of the population divided by n. Therefore, the larger the sample size of the distribution, the smaller the variance of the sample mean

# Compare with part A
The part A's instogram will plot the number of successfull attacks of each system analized, however the CLT is not directly visible from the charts. To "obtain an asymptotic convergence to a proper distribution" the instogram should display for each system the **average** of successfull attacks (num_breaches/tot_attacks) and also use a large number of samples, then we should see a proper "bell curve"; the data should also be **Normalized**. <br> 
Normalize typically involves transforming data to have a standard mean and standard deviation, which can be especially useful when comparing data from different sources or when dealing with variables on different scales. A common form of normalization is **Z-score** normalization.
$$
Z= \frac{X-\mu}{\sigma}
$$
+ z is the standardized value (z-score).
+ x is the original data point.
+ μ is the mean of the data.
+ σ is the standard deviation of the data.

The connection between normalization and the CLT lies in the idea that when we normalize data, we are making them conform to a standard normal distribution with a mean of 0 and a standard deviation of 1. This can be helpful when working with the CLT because it simplifies the process of using standard normal distribution tables or conducting statistical tests that assume normality.








> ref <br>
> https://www.statlect.com/asymptotic-theory/law-of-large-numbers <br>
> https://corporatefinanceinstitute.com/resources/data-science/central-limit-theorem/ <br>
> chat.openai.com <br>

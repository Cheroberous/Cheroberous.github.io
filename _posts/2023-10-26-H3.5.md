---
title: Statistic H_3.5
date: 2023-10-26 18:18:11 +/-0000
categories: [learning]
math: true
---


# Practical part 

Revise and optimize you previous programs to compute the joint distribution of any number of 2,3, ...k, continuous quantitative variables
where, for each variable, the user can specify the number of subdivisions ("class intervals") of a range containing the observed values.

Revise also your previous homework taking into account that qualitative variables can be ordered and therefore the order needs to be preserved.

For quantitative variables, include the possibility to specify class intervals too.

# Implementation

The prevous version of the program was integrated to support the joint of K continuous variables, 

{% include embed/youtube.html id='DPkYLyuy6Us' %}
                                  
The text field enable the user to choose how many intervals he wants for each variable, the first number refers to the first (not temporarly) selected variable in the checkbox below

![Desktop View](/assets/statiistics/H3.5/n1.png){: w="350"}

In the code we have few key functions, the first **"dividi_classi"** will calculate for each variables the different K intervals and add them to a List<(float,float):
```c#

 List<(float, float)> tuple_intervalli = new List<(float, float)>();



 List<float> l = new List<float>();

 l = Diz_tot[nome].Select(float.Parse).ToList();                        // The list used come from the initial parse of the input file

 float max = l.Max();
 
 float min = l.Min();

 float smallest = l.Max();
 float secondSmallest=l.Max();

 if (nome == "height" || nome == "weight")                   // the value 0 cannot be consider a valid minimum for these two variables so i need to calculate the net smallest value
 {
     foreach (float number in l)
     {
         if (number < smallest)
         {
             secondSmallest = smallest;
             smallest = number;
         }
         else if (number < secondSmallest && number != smallest)
         {
             secondSmallest = number;
         }
     }
     min = secondSmallest;
 }


 float dim = (max - min) / (float)intervalli;                          // calcolo grandezza ogni intervallo




 for (int i = 0; i < intervalli; i++)                   // alcolo ogni intervallo
 {
     float s = min + i * dim;
     float s1 = s + dim;
     intervals_quanti.Add((s, s1), 0);

     tuple_intervalli.Add((s, s1));                          // aggiungo l'intervallo alla lista degli intervalli

     

 }

```
The second one **"check_interval"** will check each row and associate for each of the attribute of interest which is the associated interval 
```c#

float valore1 = float.Parse(valore);          // il valore arriva come stringa

string init="";
string fin="";
string s = "";



foreach (var interval in totale_var_int[index])       
{
    int intervallo = totale_var_int[index].Count-1;
    if (valore1 == totale_var_int[index][intervallo].Item2)                 // check specifico se il valore è l'ultimo nell'ultmo intervallo (altrimenti andrebbe perso)
    {
        init = totale_var_int[index][intervallo].Item1.ToString();
        fin = totale_var_int[index][intervallo].Item2.ToString();

        s = "("+init + ";" + fin + ")";
        return s;

    }

    if (valore1 >= interval.Item1 && valore1 < interval.Item2)                      //  controllo a quale intervallo il valore appariene e 
    {
         s = "("+interval.Item1.ToString() + ";" + interval.Item2.ToString()+")";

        init = interval.Item1.ToString();
        fin = interval.Item2.ToString();
        return s;
    }
    
}

s = init + "," + fin;

return s;
```
After finding all the related k intervals a third function will add a +1 to a key in a dictionary build like this:  "key=interval_variable_1,interval_variable_2,.. value=how_many" (and repeat the process for each row)

## Qualitatives variables
Qualitative variables don't have numerical values, so we cannot directly divide them in k class intervals, we need to choose which method use. Here are some common approaches:

1.Frequency-Based Intervals: <br>
One way to divide qualitative variables is based on their frequency of occurrence. we can group categories that occur most frequently together and create intervals. This approach works well when we want to simplify the representation of data without losing too much information.

1.Expert Judgment: <br>

Sometimes, experts or domain knowledge can help determine meaningful intervals for qualitative variables. For instance, in a survey where people rate their satisfaction with a product, you might decide that "Very Dissatisfied" and "Dissatisfied" can be grouped together to form a "Low Satisfaction" category.

1.Cluster Analysis: <br>

If you have a large number of categories within a qualitative variable, we can use cluster analysis to group similar categories together. This method requires defining a distance metric to measure the dissimilarity between categories. Categories with small distances can be grouped into the same interval.

1.Ordinal Variables: <br>

If the qualitative variable has a natural order (e.g., low, medium, high), we can use this order to define intervals. For example, we can group "low" and "medium" into one interval and "high" into another. <br>
<br>

Once the method is choosen we can build an ad-hoc algorithm that take into account the different classes. <br>

Similar considerations have to be made for the **order** of those attribute's values. Natural Order? Ordinar variables? Again, a specific request need to be expressed to perform "ordering" in this circumstance.



# Theory 

Research

(Revise and improve your simulations in homework 3, where necessary.)
Search on the web about the Law of large numbers LLN and compare it with Part b of your homework 3 and express in your own words whether your simulation is somehow related with this theorem, and why.
Search on the web about the Central Limit Theorem CLT and compare it with Part a of your homework 3 and say in your own words whether your simulation is somehow related with this theorem, and why.
Based on the CLT, how could you modify ("normalize") the "security score" to obtain an asymptotic convergence to a proper distribution?

## LLN (tanti sistemi anchi pochi sample = circa stesso risultato)
The law of large numbers states that even random events with a large number of trials may return stable long-term results. Note that the theorem deals only with a large number of trials while the average of the results of the experiment repeated a small number of times might be substantially different from the expected value. <br>

A Law of Large Numbers (LLN) is a proposition that provides a set of sufficient conditions for the convergence of the sample mean to a constant.

Let {Xn} be a sequence of random variables.

Let Xn be the sample mean of the first n terms of the sequence:

$$
  \overline{X_n} =  \frac{1}{n}\sum_{i=1}^n X_i  
$$
<br>
A Law of Large Numbers (LLN) states some conditions that are sufficient to guarantee the convergence of Xn to a constant, as the sample size n increases.

Typically, the constant is the expected value of the distribution from which the sample has been drawn:

![g1](/assets/statiistics/H3.5/G1.gif)
_demo_

Typically, all the random variables in the sequence { $X_n$ } have the same expected value  $E\left[ X_n \right] = \mu$ . In this case, the constant to which the sample mean converges is mu (which is called population mean).

#Example of Law of Large Numbers

The simplest example of the law of large numbers is rolling the dice. The dice involves six different events with equal probabilities. The expected value of the dice events is:

$$
EV=\frac{1+2+3+4+5+6}{6}=3.5
$$

 

If we roll the dice only three times, the average of the obtained results may be far from the expected value. Let’s say you rolled the dice three times and the outcomes were 6, 6, 3. The average of the results is 5. According to the law of the large numbers, if we roll the dice a large number of times, the average result will be closer to the expected value of 3.5.

## COMPARE WITH PART B (modifica)

The part B of the previous homework asked to "simulate the cumulated frequency, say f, of penetration. Do the same with the relative..." , there the probability of fail/success was fixed and chosen by the user so the shape that will appear is not related to the LLN (infact the result is not a uniform distributon) but will be a bell curve.

## CLT

The Central Limit Theorem (CLT) is a statistical concept that states that the sample mean distribution of a random variable will assume a near-normal or normal distribution if the sample size is large enough. In simple terms, the theorem states that the sampling distribution of the mean approaches a normal distribution as the size of the sample increases, regardless of the shape of the original population distribution. <br>


![g2](/assets/statiistics/h3/normal.png)
_demo_

As the user increases the number of samples to 30, 40, 50, etc., the graph of the sample means will move towards a normal distribution. The sample size must be 30 or higher for the central limit theorem to hold.

One of the most important components of the theorem is that the **mean of the sample will be the mean of the entire population**. If you calculate the mean of multiple samples of the population, add them up, and find their average, the result will be the estimate of the population mean. <br>

# How Does the Central Limit Theorem Work? 
The central limit theorem forms the basis of the probability distribution. It makes it easy to understand how population estimates behave when subjected to repeated sampling. When plotted on a graph, the theorem shows the shape of the distribution formed by means of repeated population samples.

As the sample sizes get bigger, the distribution of the means from the repeated samples tends to normalize and resemble a normal distribution. The result remains the same regardless of what the original shape of the distribution was. It can be illustrated in the figure below:

![g2](/assets/statiistics/H3.5/bell.png)
_demo_

From the figure above, we can deduce that despite the fact that the original shape of the distribution was uniform, it tends towards a normal distribution as the value of n (sample size) increases.
Apart from showing the shape that the sample means will take, the central limit theorem also gives an overview of the mean and variance of the distribution. The sample mean of the distribution is the actual population mean from which the samples were taken.
The variance of the sample distribution, on the other hand, is the variance of the population divided by n. Therefore, the larger the sample size of the distribution, the smaller the variance of the sample mean

# Compare with part A
The part A's instogram will plot the number of successfull attacks of each system analized, however the CLT is not directly visible from the charts. To "obtain an asymptotic convergence to a proper distribution" the instogram shoul display fro each system the **average** of successfull attacks (num_breaches/tot_attacks) and also use a large number of samples








> ref
> https://www.statlect.com/asymptotic-theory/law-of-large-numbers
> https://corporatefinanceinstitute.com/resources/data-science/central-limit-theorem/
> chat.openai.com

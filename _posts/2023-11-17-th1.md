---
title: Statistic th1
date: 2023-11-17 8:51:11 +/-0000
categories: [learning]
math: true
---

# The LLN Meaning, Proof, Simulations

The law of large numbers states that even random events with a large number of trials may return stable long-term results. Note that the theorem deals only with a large number of trials while the average of the results of the experiment repeated a small number of times might be substantially different from the expected value. <br>

A Law of Large Numbers (LLN) is a proposition that provides a set of sufficient conditions for the convergence of the sample mean to a constant.

Let {Xn} be a sequence of random variables.

Let Xn be the sample mean of the first n terms of the sequence:

$$
  \overline{X_n} =  \frac{1}{n}\sum_{i=1}^n X_i  
$$
<br>
A Law of Large Numbers (LLN) states some conditions that are sufficient to guarantee the convergence of Xn to a constant, as the sample size n increases.

Typically, the constant is the expected value of the distribution from which the sample has been drawn:

![g1](/assets/statiistics/H3.5/G1.gif){: h ="250}
_demo_

Typically, all the random variables in the sequence { $X_n$ } have the same expected value  $E\left[ X_n \right] = \mu$ . In this case, the constant to which the sample mean converges is mu (which is called population mean).

#Example of Law of Large Numbers

The simplest example of the law of large numbers is rolling the dice. The dice involves six different events with equal probabilities. The expected value of the dice events is:

$$
EV=\frac{1+2+3+4+5+6}{6}=3.5
$$

 

If we roll the dice only three times, the average of the obtained results may be far from the expected value. Letâ€™s say you rolled the dice three times and the outcomes were 6, 6, 3. The average of the results is 5. According to the law of the large numbers, if we roll the dice a large number of times, the average result will be closer to the expected value of 3.5.


## Weak LLN

Weak Laws
A LLN is called a Weak Law of Large Numbers (WLLN) if the sample mean converges in probability.

The adjective weak is used because convergence in probability is often called weak convergence. It is employed to make a distinction from Strong Laws of Large Numbers, in which the sample mean is required to converge almost surely.

![g1](/assets/statiistics/th1/th1-1.png){: h="150"}
_demo_

**One of the best known WLLNs is Chebyshev's.**

Proposition (Chebyshev's WLLN) Let the below wquation be an uncorrelated and covariance stationary sequence:

$\exists\mu\in R :E\left[X_n \right] = \mu, \forall n\in N $ <br>
$\exists \theta^2 \in R_+ : Var\left[X_n \right]= \theta^2, \forall n \in N $ <br>
$Cov\left[X_n,X_n+1 \right] = 0, \forall n,k \in N $

Then, a Weak Law of Large Numbers applies to the sample mean:
$$
\lim\limits_{n \to \infty} \overline{X_n} =\mu
$$
where lim denotes a probability limit.

## Proof

Note that it is customary to state Chebyshev's Weak Law of Large Numbers as a result on the convergence in probability of the sample mean: <br>
$ \lim\limits_{n \to \infty} \overline{X_n} =\mu $
However, the conditions of the above theorem guarantee the mean square convergence of the sample mean to mu: <br>
$ \overline{X_n} \to \mu $
Hence, in Chebyshev's WLLN, **convergence in probability is just a consequence of the fact that convergence in mean square implies convergence in probability**.



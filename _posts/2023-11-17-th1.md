---
title: Statistic th1
date: 2023-11-17 8:51:11 +/-0000
categories: [learning]
math: true
---

# The LLN Meaning, Proof, Simulations

The law of large numbers states that even random events with a large number of trials may return stable long-term results. Note that the theorem deals only with a large number of trials while the average of the results of the experiment repeated a small number of times might be substantially different from the expected value. <br>

A Law of Large Numbers (LLN) is a proposition that provides a set of sufficient conditions for the convergence of the sample mean to a constant.

Let {Xn} be a sequence of random variables.

Let Xn be the sample mean of the first n terms of the sequence:

$$
  \overline{X_n} =  \frac{1}{n}\sum_{i=1}^n X_i  
$$
<br>
A Law of Large Numbers (LLN) states some conditions that are sufficient to guarantee the convergence of Xn to a constant, as the sample size n increases.

Typically, the constant is the expected value of the distribution from which the sample has been drawn:

![g1](/assets/statiistics/H3.5/G1.gif)
_demo_

Typically, all the random variables in the sequence { $X_n$ } have the same expected value  $E\left[ X_n \right] = \mu$ . In this case, the constant to which the sample mean converges is mu (which is called population mean).

#Example of Law of Large Numbers

The simplest example of the law of large numbers is rolling the dice. The dice involves six different events with equal probabilities. The expected value of the dice events is:

$$
EV=\frac{1+2+3+4+5+6}{6}=3.5
$$

 

If we roll the dice only three times, the average of the obtained results may be far from the expected value. Let’s say you rolled the dice three times and the outcomes were 6, 6, 3. The average of the results is 5. According to the law of the large numbers, if we roll the dice a large number of times, the average result will be closer to the expected value of 3.5.


## Weak LLN

Weak Laws
A LLN is called a Weak Law of Large Numbers (WLLN) if the sample mean converges in probability.

The adjective weak is used because convergence in probability is often called weak convergence. It is employed to make a distinction from Strong Laws of Large Numbers, in which the sample mean is required to converge almost surely.

![g1](/assets/statiistics/th1/th1-1.png){: h="250"}
_demo_

**One of the best known WLLNs is Chebyshev's.**

Proposition (Chebyshev's WLLN) Let [eq1] be an uncorrelated and covariance stationary sequence:

$$
\exists\mu\in R :E\left[X_n \right] = \mu, \forall n\in N 
\exists \theta^2 \in R_+ : Var\left[X_n \right]= \theta^2, \forall n \in N 
Cov\left[X_n,X_n+1 \right] = 0, \forall n,k \in N
$$

Then, a Weak Law of Large Numbers applies to the sample mean:
$$
\lim\limits_{n \to \infty} \overline{X_n} =\mu
$$
where lim denotes a probability limit.

## proof

Note that as stated above, for the SLUN we only require that the first moment E [Yil defined and
finite. Proving the SSLN under these conditions is a very long proof and probably overkill for the
audience of this article. Therefore, to facilitate the derivation of a more "simple" proof, we will make the
more restricted assumption that we also need the second and fourth moments to be defined and finite,
i.e. E[Yi21 < and E[Yi41 <
Note we will also assume, with no loss of generality, that the fixed is equal to zero, g = O. We can
allow this by just considering a transformation of random variables, where if were non-zero we could
always specify a new set of random variables IVI = — g for allj, with E[Wil = = O, You Will see
in a moment why this specification makes our proof easier and more manageable. So let's just simply
specify = O for convenience.
Now consider the scenario where the limit of Xit as n -ò cn is not equal to = O:

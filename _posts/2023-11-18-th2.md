---
title: Statistic th2
date: 2023-11-18 11:12:11 +/-0000
categories: [learning]
math: true
---

## The CLT Meaning, Proof, Simulations

The Central Limit Theorem (CLT) is a statistical concept that states that the sample mean distribution of a random variable will assume a near-normal or normal distribution if the sample size is large enough. In simple terms, the theorem states that the sampling distribution of the mean approaches a normal distribution as the size of the sample increases (as also shown in the "simulation" part in the th1), regardless of the shape of the original population distribution. <br>


![g2](/assets/statiistics/h3/normal.png){: w="400", : h="300"}
_demo_

As the user increases the number of samples to 30, 40, 50, etc., the graph of the sample means will move towards a normal distribution. The sample size must be 30 or higher for the central limit theorem to hold. <br>
One of the most important components of the theorem is that the **mean of the sample will be the mean of the entire population**. If you calculate the mean of multiple samples of the population, add them up, and find their average, the result will be the estimate of the population mean. <br>

# How Does the Central Limit Theorem Work? 
The central limit theorem forms the basis of the probability distribution. It makes it easy to understand how population estimates behave when subjected to repeated sampling. When plotted on a graph, the theorem shows the shape of the distribution formed by means of repeated population samples.

As the sample sizes get bigger, the distribution of the means from the repeated samples tends to normalize and resemble a normal distribution. The result remains the same regardless of what the original shape of the distribution was. It can be illustrated in the figure below:

![g2](/assets/statiistics/H3.5/bell.png){: h="250"}
_demo_

From the figure above, we can deduce that despite the fact that the original shape of the distribution was uniform, it tends towards a normal distribution as the value of n (sample size) increases. <br>
Apart from showing the shape that the sample means will take, the central limit theorem also gives an overview of the mean and variance of the distribution. <br>
The sample mean of the distribution is the actual population mean from which the samples were taken. <br>
The variance of the sample distribution, on the other hand, is the variance of the population divided by n. Therefore, the larger the sample size of the distribution, the smaller the variance of the sample mean

## Componens of the CLT

As the sample size increases, the sampling distribution of the mean, X-bar, can be approximated by a normal distribution with mean µ and standard deviation σ/√n where:
+ µ is the population mean
+ σ is the population standard deviation
+ n is the sample size

Suppose we draw a random sample of size n (x1, x2, x3, … xn — 1, xn) from a population random variable that is distributed with mean µ and standard deviation σ.

Do this repeatedly, drawing many samples from the population, and then calculate the $\overline X $ of each sample. <br>

We will treat the $ \overline X $ values as another distribution, which we will call the sampling distribution of the mean. <br>

Given a distribution with a mean μ and variance $ σ^2 $, the sampling distribution of the mean approaches a normal distribution with a mean (μ) and a variance $\frac{σ^2}{n} $ as n, the sample size, increases and the amazing and very interesting intuitive thing about the central limit theorem is that no matter what the shape of the original (parent) distribution, the sampling distribution of the mean approaches a normal distribution.

A normal distribution is approached very quickly as n increases (note that n is the sample size for each mean and not the number of samples).

Remember, in a sampling distribution of the mean the number of samples is assumed to be infinite.

To wrap up, there are three different components of the central limit theorem:
+ Successive sampling from a population
+ Increasing sample size
+ Population distribution

## Proof (Using cumulants)


The moment generating function is not always defined, since the implicit integral E[e tX] need not converge, in general. However, the following trick
will ensure that the integral will always converge: simply make t imaginary! This prompts the definition of the characteristic function <br>
![image](https://github.com/Cheroberous/cheroberous.github.io/assets/102479391/ccc7c0d8-1342-421f-b56a-b5edc2f13b06) <br>

Here the integrand is bounded so the integral always converges (since E[1] = 1). The astute reader will notice that ϕX is just the Fourier transform of X
(in an appropriate sense). Therefore, we would expect that convergence in terms of characteristic functions implies convergence in distribution, since the
inverse Fourier transform is continuous. This is just the contents of L´evy’s continuity theorem! Hence, to make our proof completely formal, all we need
to do is make the argument t imaginary instead of real. The classical proof of the central limit theorem in terms of characteristic functions argues directly
using the characteristic function, i.e. without taking logarithms. Suppose that the independent random variables Xi with zero mean and variance $ σ^2 $ have bounded third moments. Thus <br>
![image](https://github.com/Cheroberous/cheroberous.github.io/assets/102479391/3dfa7a64-201a-4ca4-aa7c-01ca928d6ac3) <br>
Using the identities for the moment generating function <br>
![image](https://github.com/Cheroberous/cheroberous.github.io/assets/102479391/63e056ae-9554-464d-8b01-05502fe0c550) <br>
The righthand side is just the characteristic function of a normal variable, so the proof is concluded with an application of L´evy’s continuity theorem.








Ref.
>https://www.cs.toronto.edu/~yuvalf/CLT.pdf
>https://builtin.com/data-science/understanding-central-limit-theorem
>https://corporatefinanceinstitute.com/resources/data-science/central-limit-theorem/ <br>

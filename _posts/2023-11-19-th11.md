---
title: Statistic th11
date: 2023-11-19 16:18:11 +/-0000
categories: [learning]
math: true
---

# The functional CLT (Donsker's invariance principle): Proof, Simulations


In probability theory, Donsker's theorem (also known as Donsker's invariance principle, or the functional central limit theorem), named after Monroe D. Donsker, is a functional extension of the central limit theorem.

Let $X_1, X_2, X_3, \ldots$ be a sequence of independent and identically distributed (i.i.d.) random variables with mean 0 and variance 1. Let  $ S_n:=\sum_{i=1}^n X_i$.
The stochastic process $S:=(S_n)_{n\in\N} $ is known as a random walk. Define the diffusively rescaled random walk (partial-sum process) by $W^{(n)}(t) := \frac{S_{\lfloor nt\rfloor}}{\sqrt{n}}, \qquad t\in [0,1]$.
The central limit theorem asserts that $W^{(n)}(1)$ converges in distribution to a standard Gaussian random variable W(1) as $n\to \infty$ . Donsker's invariance principle[1][2] extends this convergence to the whole function $ W^{(n)}:=(W^{(n)}(t))_{t\in [0,1]}$. More precisely, in its modern form, Donsker's invariance principle states that: As random variables taking values in the Skorokhod space 
$ \mathcal{D}[0,1] $ the random function $W^{(n)}$ converges in distribution to a standard Brownian motion $W:=(W(t))_{t\in [0,1]}$ as $n\to \infty$.

## Proof Sketch:
The proof involves showing that the finite-dimensional distributions of Sn(t) converge to those of B(t). The key steps include:

+ Characteristic Functions: Use characteristic functions to show convergence in distribution.
+ Convergence of Finite-Dimensional Distributions: Establish the convergence of the finite-dimensional distributions of Sn(t) to those of B(t).
+ Tightness and Prokhorov's Theorem: Show tightness of the sequence of processes, and then apply Prokhorov's theorem to conclude weak convergence.

For more in-depth details plese visit: zbr>
+ https://www.math.utah.edu/~davar/ps-pdf-files/donsker.pdf <br>
  and <br>
+ https://math.uchicago.edu/~may/REU2019/REUPapers/Schondorf.pdf

## Simultaion 
```python
import numpy as np
import matplotlib.pyplot as plt

# Parameters
n = 1000  # Number of random variables
m = 1000  # Number of paths
t_values = np.linspace(0, 1, 1000)  # Time values

# Generate random variables and compute partial sum processes
X = np.random.normal(size=(n, m))
partial_sums = np.cumsum(X - np.mean(X, axis=0), axis=0) / np.sqrt(n)

# Plot sample paths and compare to Brownian motion
plt.figure(figsize=(10, 6))
for i in range(m):
    plt.plot(t_values, partial_sums[:, i], alpha=0.1, color='blue')

# Add Brownian motion for comparison
brownian_motion = np.random.normal(size=len(t_values))
brownian_motion = np.cumsum(brownian_motion) / np.sqrt(len(t_values))
plt.plot(t_values, brownian_motion, color='red', linewidth=2, label='Brownian Motion')

plt.title("Donsker's Invariance Principle Simulation")
plt.xlabel('Time')
plt.ylabel('Value')
plt.legend()
plt.show()
```

![image](https://github.com/Cheroberous/cheroberous.github.io/assets/102479391/c24736cb-bdc3-4945-ba31-4d53606699ae) <br>

In this simulation, we generate multiple paths of the normalized partial sum process and compare them to a standard Brownian motion. As the number of random variables increases, you should observe convergence in distribution according to Donsker's Invariance Principle. Adjust the parameters to explore different scenarios.

Ref
>chat.openai
>https://en.wikipedia.org/wiki/Donsker%27s_theorem















---
title: Statistic th7
date: 2023-11-19 18:00:11 +/-0000
categories: [learning]
math: true
image: /assets/img/previews/im4.jpeg
---

# "Online" Algorithms (Data Streams): Ideas and code

# Data Streams 
Efficiently handling vast datasets has become increasingly crucial in recent decades, driven by the widespread availability of substantial data volumes across various computational science applications. A significant data management challenge has arisen, particularly in monitoring extensive and rapidly changing data streams that continuously emerge. This issue is prominent in diverse applications, including the analysis of network traffic, online auctions, transaction logs, telephone call records, automated bank machine operations, as well as atmospheric and astronomical events. Consequently, the streaming model has garnered considerable attention. This model distinguishes itself from traditional computation over stored datasets, as algorithms are required to process input through either a single pass or a limited number of passes, utilizing a constrained amount of working memory. The streaming model is applicable in scenarios where the input size far surpasses the available main memory, and the practical means of accessing the data involves making one or more passes over it. <br>
<br>
![image](https://github.com/Cheroberous/cheroberous.github.io/assets/102479391/32e3a7ab-688a-4236-8677-65baf44c0582) <br>
<br>




## Applications 
The primary application of data stream algorithms is to monitorcontinuously huge and rapidly changing streams of data in order to support exploratory
analyses and to detect correlations, rare events, fraud, intrusion, and unusual or anomalous activities (as in **IDS** and **IPS**). Such streams of data may be, for example, performance measurements in traffic management, all detail records in telecommunications, transactions in retail chains, ATM operations in banks, bids in online auctions (Ordering in an asynchronous network is an interesting problem thet requires ad-hoc algorithms), log records generated by Web Servers, or sensor network data. In all these cases, the volumes of data are huge (several terabytes or even petabytes), and records arrive at a rapid rate. Other relevant applications for data stream processing are related, for example, to processing massive files on secondary storage and to monitoring the contents of large databases or data warehouse environments.

###  Network Management 
Perhaps the most prominent application is related to network management. This involves monitoring and configuring network
hardware and software to ensure smooth operations. Consider, for example, traffic
analysis in the Internet. Here, as IP packets flow through the routers, we would like
to monitor link bandwidth usage, to estimate traffic demands, to detect faults, congestion, and usage patterns. Typical queries that we would be able to answer are thus
the following. How many IP addresses used a given link in a certain period of time?
How many bytes were sent between a pair of IP addresses? Which are the top 100 IP
addresses in terms of traffic? What is the average duration of an IP session? Which
sessions transmitted more than 1000 bytes? Which IP addresses are involved in more
than 1000 sessions? All these queries are heavily motivated by traffic analysis, fraud
detection, and security.
To get a rough estimate of the amount of data that need to be analyzed to answer
one such query, consider that each router can forward up to 1 billion packets per hour,
and each Internet Service Provider may have many hundreds of routers: thus, many
terabytes of data per hour need to be processed.

### Online Auctions 
During the last few years, online implementations of
auctions have become a reality, thanks to the Internet and to the wide use of computermediated communication technologies. In an online auction system, people register
to the system, open auctions for individual items at any time, and then submit continuously items for auction and bids for items. Statistical estimation of auction data is
thus very important for identifying items of interest to vendors and purchasers, and
for analyzing economic trends.


##  Data Stream Models

The input stream a1, a2,... arrives sequentially, item by item, and describes an underlying signal A, a onedimensional function A : [1 ...N] → R. Input may comprise multiple streams or multidimensional signals, but we do not consider those variations for now. Models differ on how the ai’s describe A. <br>
+ **Time Series Model**. Each ai equals A[i] and they appear in increasing order of i. This is a suitable
model for time series data where, for example, you are observing the traffic volume at an IP link
every 5 minutes, or NASDAQ volume of trades each minute, etc. At each such time “period”, we
observe the next new update. <br>
![image](https://github.com/Cheroberous/cheroberous.github.io/assets/102479391/9d264b3f-329d-4691-9a9a-377fdd6f3acd) <br>

+ **Cash Register Model**. Here ai’s are increments to A[j]’s. Think of ai = (j, Ii), Ii ≥ 0, to mean
Ai[j] = Ai−1[j] + Ii where Ai is the state of the signal after seeing the ith item in the stream.
Much as in a cash register, multiple ai’s could increment a given A[j] over time.3 This is perhaps
the most popular data stream model. It fits applications such as monitoring IP addresses that
access a web server, source IP addresses that send packets over a link etc. because the same IP
addresses may access the web server multiple times or send multiple packets on the link over
time. This model has appeared in literature before, but was formally christened in [102] with this
name. A special, simpler case of this model is when ai’s are an arbitrary permutation of A[j]’s,
that is, items do not repeat in the stream, but they appear out of order.
+ **Turnstile Model**. Here ai’s are updates to A[j]’s. Think of ai = (j, Ii), to mean Ai[j] =
Ai−1[j] +Ii where Ai is the signal after seeing the ith item in the stream, and Ii may be positive
or negative. This is the most general model. It is mildly inspired by a busy NY subway train station where the turnstile keeps track of people arriving and departing continuously. At any time, a
large number of people are in the subway. This is the appropriate model to study fully dynamic
situations where there are inserts as well deletes, but it is often hard to get powerful bounds in
this model. This model too has appeared before under different guises, but it gets christened here


# Streaming Algorithms
The size of the stream is denoted by m, and the size of the universe of all events is denoted by n. The element of a stream is modeled as a tuple. The simplest tuple is <element_id, frequency>. In the most basic case, if the frequency is not specified, it defaults to 1.
The central goal of streaming algorithms is to process the input stream using a small amount of space, that is, to use s bits of random access working memory. Since m and n are huge, we want to make s much smaller than these. Specifically, we want s to be sublinear in both m and n.

An important thing to note is that the output of streaming algorithms computations is approximate. The inductive reasoning for this is that you cannot exclude any element in the stream to compute exact results. As a result, you end up keeping all the elements that translate to a linear space requirement, proportional to either m or n.

Streaming algorithms are also classified as: <br>

1. **Single-Pass**: the algorithm can make only one pass over the data set. Single-pass algorithms comprise the following three phases.
 - Initialization: This is where we do initialization before processing the stream.
 - Processing: executed each time we see an element from the stream.
 - Output/Query: This is where we answer questions about the stream.
1. **Multi-Pass**: the algorithm can make multiple passes over the data set. <br>

**Let's now look at some of the simplest streaming algorithms**. <br>

### Majority Element — Boyer- Moore Algorithm
The Boyer–Moore majority algorithm is used for finding the majority element in a sequence using linear time and constant space. It was published in 1981 by Robert S. Boyer and J Strother Moore. A majority element is defined as an element that occurs strictly greater than n/2, where n is the size of the sequence. The single-pass Boyer Moore algorithm finds a majority element if one exists. Else the result can be any arbitrary element. A second pass can guarantee that the element returned is the majority element.

The pseudocode below shows the steps.

Pass 1

1. **Initialization**: Assign a candidate, initially null and a counter = 0
2. **Processing**: For each element x in the sequence <br>
if counter is 0, set candidate = x and increment counter
else if x == candidate increment counter
else decrement counter
3. **Output/Query**: Return the potential candidate.

Pass 2

Reset counter to 0.
For each element x in the sequence
if candidate == x increment counter
return candidate if counter > n/2 else return null.
Note that the second pass is required to determine if the candidate is the majority element. The algorithm will report one of the sequence elements with a single pass, even if there is no majority element.

### Misra-Gries Algorithm
In the majority algorithm above, we were looking for a majority candidate. A majority candidate was defined as an element that occurs more than n/2 times where n is the size of the sequence. The generalized form of the majority problem is called the Frequency problem. Given a parameter k, output the set of all elements with a frequency greater than n/k, where n is the size of the sequence.

The Misra-Gries Algorithm maintains an associative array, A, whose keys are elements seen in the stream and whose values are counters associated with the elements. It keeps at most k −1counters at any time. First, let us look at the pseudocode for the algorithm.

Pass 1

1. **Initialize**: A (empty associative array)
2. **Process element** x in stream:

if x ∈ keys(A) then increment the counter for x in A
else if total_keys in A < k − 1 then set A[x] = 1
else
foreach l in keys(A) do
decrement the counter for l in A
if A[l] = 0 then remove l from A
Pass 2

Output: On query a, if a ∈ keys(A), make a second pass on the sequence to get frequency, else return null
The counter A[x] is incremented only when we process an occurrence of x in the stream. On the other hand, whenever the total keys in the associative array exceed (or equals) k, we decrement k other counters, corresponding to distinct elements in the stream. Thus, each decrement is witnessed by a collection of k distinct elements from the stream. Since the stream consists of m elements, there can be at most m/k such decrements.

If some element x has frequency > m/k, then its corresponding counter A[x] will be positive at the end of the first pass over the stream; that is, x will be in keys(A). Thus, we can make a second pass over the input stream, counting exactly the frequencies for all x ∈ keys(A), and output the desired set of items.


> _For the implementation details plese visit_ [Link](https://github.com/maneeshchaturvedi/streaming-algorithms).
{: .prompt-info }


# Sampling
A small random sample S of the data often captures certain characteristics of the entire data set (See also "link" which is a paper written by me and other collegues about user-identification through data stream feature extraction related with the visited website). If this is the case, the sample can be maintained in memory and queries can be answered over the sample. In order to use sampling techniques in a data stream context, we first need to address the problem of maintaining a sample of a specified
size over a possibly unbounded stream of data that arrive online. The standard solution is to use **Vitter’s reservoir sampling**.

###  Reservoir Sampling This technique dates back to the 1980s. Given a stream  of n items that arrive online, at any instant of time reservoir sampling guarantees to maintain a uniform random sample S of fixed size m of the part of
stream observed up to that time. Let us first consider the following natural sampling procedure.
At the beginning, add to S the first m items of the stream. Upon seeing the stream item xt at time t, add xt to S wi

## Inference in data Streams
A simple application of sampling is to address the problem of identifying frequent items in a data stream, that is, items whose frequency exceeds a user-specified threshold. Intuitively, it should be possibleto estimate frequent items by a good sample. <br>
The algorithm accepts two user-specified thresholds: a frequency threshold ϕ ∈ (0, 1), and an error parameter ε ∈ (0,1) such that ε<ϕ. <br>
Let  be a stream of n items x1, ..., xn. The goal is to report
 + all the items whose frequency is at leastϕ n (i.e., there must be no false negatives);                           
 + no item with frequency smaller than (ϕ − ε)n
We will denote by f (x) the true frequency of an item x, and by fe(x) the frequency
estimated by sticky sampling. The algorithm also guarantees small error in individual
frequencies; that is, the estimated frequency is less than the true frequency by at most
ε n. The algorithm is randomized, and in order to meet the two goals with probability at
least 1 − δ, for a user-specified probability of failure δ ∈ (0, 1), it maintains a sample
with expected size 2ε−1 log(ϕ−1δ−1) = 2t. Note that the space is independent of the
stream length n.
The sample S is a set of pairs of the form (x, fe(x)). In order to handle potentially unbounded streams, the sampling rate r is not fixed, but is adjusted so that the
probability 1/r of sampling a stream item decreases as more and more items are
considered. Initially, S is empty and r = 1. For each stream item x, if x ∈ S, then
fe(x) is increased by 1. Otherwise, x is sampled with rate r, that is, with probability
1/r: if x is sampled, the pair (x, 1) is added to S, otherwise we ignore x and move to
the next stream item.
After sampling with rate r = 1 the first 2t items, the sampling rate increases geometrically as follows: the next 2t items are sampled with rate r = 2, the next 4t items
with rate r = 4, the next 8t items with rate r = 8, and so on. Whenever the sampling
rate changes, the estimated frequencies of sample items are adjusted so as to keep
them consistent with the new sampling rate: for each (x, fe(x)) ∈ S, we repeatedly
toss an unbiased coin until the coin toss is successful, decreasing fe(x) by 1 for each
unsuccessful toss. We evict (x, fe(x)) from S if fe(x) becomes 0 during this process.
Effectively, after each sampling rate doubling, S is transformed to exactly the state it
would have been in, if the new rate had been used from the beginning.
Upon a frequency items query, the algorithm returns all sample items whose estimated frequency is at least (ϕ − ε)n.

Ref
>https://www.cs.dartmouth.edu/~ac/Teach/data-streams-lecnotes.pdf <br>
>https://www.dei.unipd.it/~geppo/PrAvAlg/DOCS/DFchapter08.pdf <br>
>https://maneesh-chaturvedi.medium.com/data-streams-models-and-algorithms-1-545de5f7f6ae#:~:text=Streaming%20Algorithms,in%20both%20m%20and%20n. <br>
>https://www.onaudience.com/resources/what-is-data-stream-and-how-to-use-it/ <br>
>https://www.udacity.com/blog/2022/07/an-introduction-to-data-streaming-technologies.html
>chat.openai.com <br>






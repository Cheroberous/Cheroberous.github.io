---
title: Statistic th7
date: 2023-11-19 18:00:11 +/-0000
categories: [learning]
math: true
image: /assets/img/previews/im4.jpeg
---

# "Online" Algorithms (Data Streams): Ideas and code

# Data Streams 
Efficiently handling vast datasets has become increasingly crucial in recent decades, driven by the widespread availability of substantial data volumes across various computational science applications. A significant data management challenge has arisen, particularly in monitoring extensive and rapidly changing data streams that continuously emerge. This issue is prominent in diverse applications, including the analysis of network traffic, online auctions, transaction logs, telephone call records, automated bank machine operations, as well as atmospheric and astronomical events. Consequently, the streaming model has garnered considerable attention. This model distinguishes itself from traditional computation over stored datasets, as algorithms are required to process input through either a single pass or a limited number of passes, utilizing a constrained amount of working memory. The streaming model is applicable in scenarios where the input size far surpasses the available main memory, and the practical means of accessing the data involves making one or more passes over it. <br>
<br>
![image](https://github.com/Cheroberous/cheroberous.github.io/assets/102479391/32e3a7ab-688a-4236-8677-65baf44c0582) <br>
<br>




## Applications 
The primary application of data stream algorithms is to monitorcontinuously huge and rapidly changing streams of data in order to support exploratory
analyses and to detect correlations, rare events, fraud, intrusion, and unusual or anomalous activities (as in **IDS** and **IPS**). Such streams of data may be, for example, performance measurements in traffic management, all detail records in telecommunications, transactions in retail chains, ATM operations in banks, bids in online auctions (Ordering in an asynchronous network is an interesting problem thet requires ad-hoc algorithms), log records generated by Web Servers, or sensor network data. In all these cases, the volumes of data are huge (several terabytes or even petabytes), and records arrive at a rapid rate. Other relevant applications for data stream processing are related, for example, to processing massive files on secondary storage and to monitoring the contents of large databases or data warehouse environments.

###  Network Management 
Perhaps the most prominent application is related to network management. This involves monitoring and configuring network
hardware and software to ensure smooth operations. Consider, for example, traffic
analysis in the Internet. Here, as IP packets flow through the routers, we would like
to monitor link bandwidth usage, to estimate traffic demands, to detect faults, congestion, and usage patterns. Typical queries that we would be able to answer are thus
the following. How many IP addresses used a given link in a certain period of time?
How many bytes were sent between a pair of IP addresses? Which are the top 100 IP
addresses in terms of traffic? What is the average duration of an IP session? Which
sessions transmitted more than 1000 bytes? Which IP addresses are involved in more
than 1000 sessions? All these queries are heavily motivated by traffic analysis, fraud
detection, and security.
To get a rough estimate of the amount of data that need to be analyzed to answer
one such query, consider that each router can forward up to 1 billion packets per hour,
and each Internet Service Provider may have many hundreds of routers: thus, many
terabytes of data per hour need to be processed.

### Online Auctions 
During the last few years, online implementations of
auctions have become a reality, thanks to the Internet and to the wide use of computermediated communication technologies. In an online auction system, people register
to the system, open auctions for individual items at any time, and then submit continuously items for auction and bids for items. Statistical estimation of auction data is
thus very important for identifying items of interest to vendors and purchasers, and
for analyzing economic trends.


##  Data Stream Models

The input stream a1, a2,... arrives sequentially, item by item, and describes an underlying signal A, a onedimensional function A : [1 ...N] → R. Input may comprise multiple streams or multidimensional signals, but we do not consider those variations for now. Models differ on how the ai’s describe A. <br>
+ **Time Series Model**. Each ai equals A[i] and they appear in increasing order of i. This is a suitable
model for time series data where, for example, you are observing the traffic volume at an IP link
every 5 minutes, or NASDAQ volume of trades each minute, etc. At each such time “period”, we
observe the next new update. <br>
![image](https://github.com/Cheroberous/cheroberous.github.io/assets/102479391/9d264b3f-329d-4691-9a9a-377fdd6f3acd) <br>

+ **Cash Register Model**. Here ai’s are increments to A[j]’s. Think of ai = (j, Ii), Ii ≥ 0, to mean
Ai[j] = Ai−1[j] + Ii where Ai is the state of the signal after seeing the ith item in the stream.
Much as in a cash register, multiple ai’s could increment a given A[j] over time.3 This is perhaps
the most popular data stream model. It fits applications such as monitoring IP addresses that
access a web server, source IP addresses that send packets over a link etc. because the same IP
addresses may access the web server multiple times or send multiple packets on the link over
time. This model has appeared in literature before, but was formally christened in [102] with this
name. A special, simpler case of this model is when ai’s are an arbitrary permutation of A[j]’s,
that is, items do not repeat in the stream, but they appear out of order.
+ **Turnstile Model**. Here ai’s are updates to A[j]’s. Think of ai = (j, Ii), to mean Ai[j] =
Ai−1[j] +Ii where Ai is the signal after seeing the ith item in the stream, and Ii may be positive
or negative. This is the most general model. It is mildly inspired by a busy NY subway train station where the turnstile keeps track of people arriving and departing continuously. At any time, a
large number of people are in the subway. This is the appropriate model to study fully dynamic
situations where there are inserts as well deletes, but it is often hard to get powerful bounds in
this model. This model too has appeared before under different guises, but it gets christened here


# Streaming Algorithms
The size of the stream is denoted by m, and the size of the universe of all events is denoted by n. The element of a stream is modeled as a tuple. The simplest tuple is <element_id, frequency>. In the most basic case, if the frequency is not specified, it defaults to 1.
The central goal of streaming algorithms is to process the input stream using a small amount of space, that is, to use s bits of random access working memory. Since m and n are huge, we want to make s much smaller than these. Specifically, we want s to be sublinear in both m and n.

An important thing to note is that the output of streaming algorithms computations is approximate. The inductive reasoning for this is that you cannot exclude any element in the stream to compute exact results. As a result, you end up keeping all the elements that translate to a linear space requirement, proportional to either m or n.

Streaming algorithms are also classified as: <br>

1. **Single-Pass**: the algorithm can make only one pass over the data set. Single-pass algorithms comprise the following three phases.
 - Initialization: This is where we do initialization before processing the stream.
 - Processing: executed each time we see an element from the stream.
 - Output/Query: This is where we answer questions about the stream.
1. **Multi-Pass**: the algorithm can make multiple passes over the data set. <br>

**Let's now look at some of the simplest streaming algorithms**. <br>

### Majority Element — Boyer- Moore Algorithm
The Boyer–Moore majority algorithm is used for finding the majority element in a sequence using linear time and constant space. It was published in 1981 by Robert S. Boyer and J Strother Moore. A majority element is defined as an element that occurs strictly greater than n/2, where n is the size of the sequence. The single-pass Boyer Moore algorithm finds a majority element if one exists. Else the result can be any arbitrary element. A second pass can guarantee that the element returned is the majority element.

The pseudocode below shows the steps.

Pass 1

1. **Initialization**: Assign a candidate, initially null and a counter = 0
2. **Processing**: For each element x in the sequence <br>
if counter is 0, set candidate = x and increment counter
else if x == candidate increment counter
else decrement counter
3. **Output/Query**: Return the potential candidate.

Pass 2

Reset counter to 0.
For each element x in the sequence
if candidate == x increment counter
return candidate if counter > n/2 else return null.
Note that the second pass is required to determine if the candidate is the majority element. The algorithm will report one of the sequence elements with a single pass, even if there is no majority element.

### Misra-Gries Algorithm
In the majority algorithm above, we were looking for a majority candidate. A majority candidate was defined as an element that occurs more than n/2 times where n is the size of the sequence. The generalized form of the majority problem is called the Frequency problem. Given a parameter k, output the set of all elements with a frequency greater than n/k, where n is the size of the sequence.

The Misra-Gries Algorithm maintains an associative array, A, whose keys are elements seen in the stream and whose values are counters associated with the elements. It keeps at most k −1counters at any time. First, let us look at the pseudocode for the algorithm.

Pass 1

1. **Initialize**: A (empty associative array)
2. **Process element** x in stream:

if x ∈ keys(A) then increment the counter for x in A
else if total_keys in A < k − 1 then set A[x] = 1
else
foreach l in keys(A) do
decrement the counter for l in A
if A[l] = 0 then remove l from A
Pass 2

Output: On query a, if a ∈ keys(A), make a second pass on the sequence to get frequency, else return null
The counter A[x] is incremented only when we process an occurrence of x in the stream. On the other hand, whenever the total keys in the associative array exceed (or equals) k, we decrement k other counters, corresponding to distinct elements in the stream. Thus, each decrement is witnessed by a collection of k distinct elements from the stream. Since the stream consists of m elements, there can be at most m/k such decrements.

If some element x has frequency > m/k, then its corresponding counter A[x] will be positive at the end of the first pass over the stream; that is, x will be in keys(A). Thus, we can make a second pass over the input stream, counting exactly the frequencies for all x ∈ keys(A), and output the desired set of items.


> _For the implementation details plese visit_ [Link](https://github.com/maneeshchaturvedi/streaming-algorithms).
{: .prompt-info }


## Some pseudo-code implementation

###  Indyk’s Algorithm (Counting Distinct Elements)

The first problem people study in the streaming setting is to approximate the frequency
moments of a data set. Let S be a sequence of items si, where each si ∈ [n] = {0, . . . , n−1}, and let: <br>

![image](https://github.com/Cheroberous/cheroberous.github.io/assets/102479391/7417fdcd-bb29-400a-939c-d3935c952d9a) <br>

be the number of occurrences of i in S. We define the k-th moment of S as <br>
![image](https://github.com/Cheroberous/cheroberous.github.io/assets/102479391/8fd352be-be99-49ec-a31a-6870b922a127) <br>

where $0^0$
is defined to be 0. By definition we have: <br>
+ F1 is the number of items in S.
+ F0 is the number of distinct items in S.

This algorithm address the problem of approximating F0 in the **turnstile stream**. Although the sampling-based algorithms are simple, they
cannot be applied in turnstile streams, and we need to develop other techniques. We first formulate the problem of approximating F0 in the turnstile stream. <br>
+ Input: A sequence of pairs of the form (si, Ui), where si ∈ [n] and Ui = +/−.
+ Output: The F0 of the data set expressed by the stream. Moreover, for parameters ε and δ, the output of the algorithm achieves an (ε, δ)-approximation.

![image](https://github.com/Cheroberous/cheroberous.github.io/assets/102479391/20345bc4-23dd-42c3-87d0-4853f01c3adf) <br>

The algorithm relies on matrix M of size k × n, and for every occurrence of item i,
the algorithm needs the ith column of matrix M. However, sublinear space cannot store
the whole matrix! So we need an effective way to generate this random matrix such that <br>
+ Every entry of M is random and generated according to the p-stable distribution.
+ Each column can be reconstructed when necessary.
To construct such matrix M, we use Nisna’s pseudorandom generators. Specifically, when
the column indexed by x is required, Nisan’s generator takes x as the input and, together
with the original see, the generator outputs a sequence of pseudorandom sequences. Based
on two consecutive pseudorandom numbers, we use (1) to generate one item.



### The BJKST Algorithm
The following is a simplifier version of the algorithm by Bar-Yossef, Jayram, Kumar, Sivakumar and Trevisan . The BJKST algorithm
uses a set to maintain the sampled items. By running Θ(log(1/δ)) independent copies in parallel and returning the medium of these outputs, **the BJKST algorithm (ε, δ)-
approximates the number of distinct items in S**.
Algorithm 2 (see below) presents a simplified version of the BJKST algorithm, where c is a constant. The general idea of the BJKST algorithm is as follows: <br>
1. Use a set B to maintain the sampled items;
1. When the set B becomes full, shrink B by removing about half items and from then on the sample probability becomes smaller.
1. In the end the number of items in B can be used to give a good approximation of the number of distinct items in S. <br>

![image](https://github.com/Cheroberous/cheroberous.github.io/assets/102479391/07163b96-f268-41b2-8686-da23084f9f03) <br>

###  Count-Min Sketch (Frequency Estimation)
Consider the following frequency estimation problem: Starting from an empty set S, there is a sequence of update operations of INSERT(S, xi) (performs S ← S ∪ {xi})
or DELETE(S, xi) (performs S ← S \ {xi}). In addition, there is a query operation QUERY(S, xi) which asks for the number of occurrences of xi
in the multiset S. The **Count-Min Sketch** is used to solve the problem above. The Count-Min Sketch is introduced by Cormode and Muthukrishnan, and consists of a
fixed array C of counters of width w and depth d. These counters are all initialized to be zero. Each row is associated to a hash function hi
, where each hi maps an element from U to {1, . . . , w}. For every INSERT(S, xi) we update C[j, h(xi)] via <br>
C[j, h(xi)] ← C[j, h(xi)] + 1 <br>

for every row 1 ≤ i ≤ d. For every DELETE(S, xi) we update C[j, h(xi)] via <br>

C[j, h(xi)] ← C[j, h(xi)] − 1 <br>

When the number of occurrences of any $ s_i$ is asked, we output: <br>
![image](https://github.com/Cheroberous/cheroberous.github.io/assets/102479391/2ed506e9-246b-4cf9-8b18-baeb537f33ad) <br>


**The figure below shows the structure of the Count-Min Sketch**. <br>

![image](https://github.com/Cheroberous/cheroberous.github.io/assets/102479391/2b33819e-90ef-4c74-bdb8-64c18871c0bc) <br>



# Sampling
A small random sample S of the data often captures certain characteristics of the entire data set (See also <https://www.overleaf.com/read/bpmgvswpwmsx#66e704> **which is a paper written by me and other colleagues about user-identification through data stream feature extraction** related with the visited website). If this is the case, the sample can be maintained in memory and queries can be answered over the sample. In order to use sampling techniques in a data stream context, we first need to address the problem of maintaining a sample of a specified
size over a possibly unbounded stream of data that arrive online. The standard solution is to use **Vitter’s reservoir sampling**.

###  Reservoir Sampling
This technique dates back to the 1980s. Given a stream  of n items that arrive online, at any instant of time reservoir sampling guarantees to maintain a uniform random sample S of fixed size m of the part of
stream observed up to that time. Let us first consider the following natural sampling procedure.
At the beginning, add to S the first m items of the stream. Upon seeing the stream item xt at time t, add xt to S wi

## Inference in data Streams
A simple application of sampling is to address the problem of identifying frequent items in a data stream, that is, items whose frequency exceeds a user-specified threshold. Intuitively, it should be possibleto estimate frequent items by a good sample. <br>
The algorithm accepts two user-specified thresholds: a frequency threshold ϕ ∈ (0, 1), and an error parameter ε ∈ (0,1) such that ε<ϕ. <br>
Let  be a stream of n items x1, ..., xn. The goal is to report
 + all the items whose frequency is at leastϕ n (i.e., there must be no false negatives);                           
 + no item with frequency smaller than (ϕ − ε)n
   
We will denote by f (x) the true frequency of an item x, and by fe(x) the frequency
estimated by sticky sampling. The algorithm also guarantees small error in individual
frequencies; that is, the estimated frequency is less than the true frequency by at most
ε n. The algorithm is randomized, and in order to meet the two goals with probability at
least 1 − δ, for a user-specified probability of failure δ ∈ (0, 1), it maintains a sample
with expected size 2ε−1 log(ϕ−1δ−1) = 2t. Note that the space is independent of the
stream length n. <br>

The sample S is a set of pairs of the form (x, fe(x)). In order to handle potentially unbounded streams, the sampling rate r is not fixed, but is adjusted so that the
probability 1/r of sampling a stream item decreases as more and more items are
considered. Initially, S is empty and r = 1. For each stream item x, if x ∈ S, then
fe(x) is increased by 1. Otherwise, x is sampled with rate r, that is, with probability
1/r: if x is sampled, the pair (x, 1) is added to S, otherwise we ignore x and move to
the next stream item. <br>

After sampling with rate r = 1 the first 2t items, the sampling rate increases geometrically as follows: the next 2t items are sampled with rate r = 2, the next 4t items
with rate r = 4, the next 8t items with rate r = 8, and so on. Whenever the sampling
rate changes, the estimated frequencies of sample items are adjusted so as to keep
them consistent with the new sampling rate: for each (x, fe(x)) ∈ S, we repeatedly
toss an unbiased coin until the coin toss is successful, decreasing fe(x) by 1 for each
unsuccessful toss. We evict (x, fe(x)) from S if fe(x) becomes 0 during this process.
Effectively, after each sampling rate doubling, S is transformed to exactly the state it
would have been in, if the new rate had been used from the beginning.
Upon a frequency items query, the algorithm returns all sample items whose estimated frequency is at least (ϕ − ε)n.

# More about data streams and feature extraction (**some excerpts from my paper**)
## Few background concepts
### Tor
Tor is an anonymous communication application whose main goal is enhancing the privacy of the users browsing the Web through it. Indeed, using the Tor browser, users can protect themselves against online tracking and surveillance, resist traffic fingerprinting and circumvent censorship.
When using Tor, Internet communications are encrypted three times and routed through circuits composed of three relays, also known as Onion Routers. <br>
![image](https://github.com/Cheroberous/cheroberous.github.io/assets/102479391/438375c5-0990-4985-9b9c-7393406dfa8c) <br>

### Website fingerprinting 
In a WF attack, the main objective of an adversary is identifying which website(s) the target is visiting.
Surveillance and intelligence purposes are the fundamental reasons that may lurk behind this: government agencies might want to fingerprint users and identify those bypassing censorship.
Typically, a **WF attack is a classification problem** in which **traffic instances** (chunk of a **data stream**) have to be categorised with respect to the websites (the labels to be assigned) they are related to.
In practice, an attacker collects traffic instances by visiting websites and trains a supervised ML model (namely, a WF model) using time, flow and packet features.
Then, whenever the target visits a website via the Tor network, the adversary collects the traffic the target has generated and feeds it to the WF model to identify the website <br>

![image](https://github.com/Cheroberous/cheroberous.github.io/assets/102479391/c1dd50d0-4c2c-4720-a1c4-d4759c82bd2c) <br>


## Main idea
Is possible to correlate the visited website by analizing the data stream ad extracting characteristic features from it. <br>
The correlation between visited website and traffic features was investigated by several papers and quite a few Machine Learning models where build to analize traffic. The accuracy achieved was high for most of the (From 60% to 90%). The main problem lies in identify those features from the datastream and build a proper ml model able to extract the most relevant ones and recognize them in the input samples. <br>

Despite the stream deriving from multiple connections **to the same site**, it will exhibit many differences introduced by network delay, the nodes through which the request has passed, the version of the browser used, which server responded, etc; so we need to make sure that traffic instances are collected using a standardized method, the choosen approach consists in visiting each website T times in a batch and considering B batches. Moreover, each batch of traffic instances is collected a **few hours apart** from the others and using a different circuit.
This process is repeated S times, each time using a different combination of _Tor version and security settings level_. <br>
_Data are extracted from a Closed Word scenario and an Open world scenario (See the paper for more details)_. <br>

## ML model
To build a model with an acceptable level of accuracy several techniques are neede, Decion Trees, pruning , random forests, bagging and so on... <br>
*Some basic concept*: <br>
### Decision tree
A decision tree model has a hierarchical tree structure
consisting of a root node, branches, internal nodes and leaf
nodes. Both root and internal nodes **conduct evaluations on
the available features to form homogenous subsets**, denoted
by leaf nodes. Therefore, decision tree learning involves
finding the optimal split points within the model (namely,
identifying the ideal attributes to split on). The splitting
process is recursively repeated in a top-down fashion until
all (or the majority of) samples have been classified under
specific labels.
In order to evaluate the quality of each test condition
(namely, how well it classifies samples under a label) and
hence the importance of each feature 3 several splitting crite-
rion for decision tree models may be considered. **Entropy,
information gain and the Gini index** are typically used as
metrics to evaluate split points. _Unfortunately, as a decision
tree grows in size, data fragmentation and overfitting might
occur . Hence, to reduce complexity and prevent overfit-
ting, either **pruning** or ensemble methods are employed_

### Random forest 
A random forest model is an ensemble made up of multiple uncorrelated decision trees that is capable of predicting more accurate results and reducing bias, variance and overfitting
issues. In order to create an uncorrelated forest of decisiontrees, the random forest algorithm uses both bagging and feature randomness.

## Building the Model

The closed-world dataset collected Dclosed is split into a
training dataset Dclosed
train and a test dataset Dclosed
test , contain-
ing roughly #Dclosed
train = ⌈0.8 · #Dclosed⌉ and #Dclosed
test = ⌊0.2 · #Dclosed⌋ samples respectively. Let K′ be the number
of features selected in Subsection 4.2 and let 0 < K′′ < K′.
Then, each of R random forest models Aclosed i∈{1,...,R} is trained on Dclosed train as follows: <br>

1) N bootstrap sample Dclosed
train,(i,1), ..., Dclosed
train,(i,N ) are
created by sampling with replacement M samples
from Dclosed
train ; <br>

2) N decision trees are trained, each one on a different
bootstrap sample Dclosed
train,(i,j), by choosing at most
from K′′ features at each split point; <br>

3) majority voting is used to aggregate results from the
N decision trees.
Each model trained uses a different combination of hyperpa-
rameters (i.e., the number of decision trees N , the size of the
training datasets M , the number of features to choose from
at each split point K′′5). Thus, the (possibly) best model
Aclosed∗ among these R random forests can be chosen by
taking the one having the smallest OOB error.
Eventually, under the closed-world assumption, the accu-
racy of Aclosed∗ is tested on Dclosed
test .
Similarly, under the open-world assumption, the best model
Aopen∗ is chosen among R random forests Aopen
i∈{1,...,R}
trained on Dclosed and its accuracy is tested on Dopen. <br>
![image](https://github.com/Cheroberous/cheroberous.github.io/assets/102479391/7cbbead8-0e47-4653-bc36-9ebdf7107131) <br>

Some other metrics to evaluate the chosen models
Aclosed∗ and Aopen∗ may be the following: <br>
+ True Positive Rate (the probability that a monitored
website is correctly classified as that particular mon-
itored website) T P R = $ \frac{TP}{TP +FN} $;
+ False Positive Rate (the probability that a non-
monitored website is incorrectly classified as a mon-
itored website) F P R = $ \frac{FP}{FP+TN} $ – especially under
the open-world assumption;
+ Precision (a measure of the correctness of positive
classifications) Pr = $\frac{TP}{TP+FP} $ 


> **If you are interested for more details look at** [My_Paper](https://www.overleaf.com/read/bpmgvswpwmsx#66e704).
{: .prompt-info }




Ref
><https://www.cs.dartmouth.edu/~ac/Teach/data-streams-lecnotes.pdf> <br>
><https://www.dei.unipd.it/~geppo/PrAvAlg/DOCS/DFchapter08.pdf> <br>
><https://maneesh-chaturvedi.medium.com/data-streams-models-and-algorithms-1-545de5f7f6ae#:~:text=Streaming%20Algorithms,in%20both%20m%20and%20n>. <br>
><https://www.onaudience.com/resources/what-is-data-stream-and-how-to-use-it/> <br>
><https://www.udacity.com/blog/2022/07/an-introduction-to-data-streaming-technologies.html> <br>
>chat.openai.com <br>
><https://www.overleaf.com/read/bpmgvswpwmsx#66e704>
><https://resources.mpi-inf.mpg.de/departments/d1/teaching/ss13/gitcs/lecture5.pdf>






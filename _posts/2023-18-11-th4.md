---
title: Statistic th4
date: 2023-11-18 12:25:11 +/-0000
categories: [learning]
math: true
---

# The Glivenko–Cantelli theorem, Proof, Simulations
The Glivenko–Cantelli theorem is a fundamental result in probability theory and mathematical statistics. It is named after Russian mathematicians Dmitry Glivenko and Francesco Cantelli. The theorem is concerned with the convergence of empirical distribution functions to the true cumulative distribution function (CDF) of a random variable.

Glivenko–Cantelli Theorem - Statement:
Let $ X_1, x_2, ..., x_N $ be independent and identically distributed random variables with cumulative distribution function (CDF) F(x) and $ F_n(x) be the empirical distribution function based on the first 
n observations. The Glivenko–Cantelli theorem states that, almost surely, <br>

$$
sup_x |F_n(x)-F(x)| \to 0
$$
<br>
as n $\to infty$ where $sup_x$ denotes the supremum over all possible values of x.

## Intuition:
In simpler terms, the theorem tells us that, as the sample size  n becomes larger, the empirical distribution function 
$ F_n$(x) converges almost surely to the true cumulative distribution function F(x). The supremum of the absolute difference between the empirical and true distribution functions approaches zero.

## Implications:
The Glivenko–Cantelli theorem is a powerful result with important implications in statistics. It provides theoretical support for using the empirical distribution function as an estimator of the true cumulative distribution function. This convergence result is strong and holds regardless of the underlying distribution of the random variables, making it a universal property.

## Applications:
The theorem is particularly useful in non-parametric statistics and the analysis of distributional properties of random variables. It plays a foundational role in understanding the behavior of sample-based estimates of distribution functions and is often used in the study of statistical convergence.

In practice, the Glivenko–Cantelli theorem is essential for establishing the consistency of empirical distribution functions as estimators of true distribution functions, providing a solid theoretical basis for statistical inference based on these estimators.

# Proof

For simplicity, consider a case of continuous random variable  X. Fix −∞=$X_0<X_1<⋯<X_{m-1}<X_m− 1<$= ∞ <br>

$ \displaystyle -\infty =x_{0}<x_{1}<\cdots <x_{m-1}<x_{m}=\infty $ such that <br>

$\displaystyle F(x_{j})-F(x_{j-1})=\frac {1}{m}$ for  j=1,... ,m. Now for all  x $\in \mathbb{R}$ there exists 

$\displaystyle j\in \{1,\dots ,m\}$ such that 

$\displaystyle x\in \left[x_{j-1},x_{j}\right]$. Note that; <br>

![image](https://github.com/Cheroberous/cheroberous.github.io/assets/102479391/d95c2c82-bd1c-48ce-b610-cd82aa5867e4) <br>

Since max $ \max _{j\in \{1,\dots ,m\}|F_{n}(x_{j})-F(x_{j})|\to 0{\text{ a.s.}}$ by strong law of large numbers, we can guarantee that for any positive 
$ \varepsilon $ and any integer m such that $ \frac{1}{m}<\varepsilon $, we can find N such that for all n>=N  we have $ \max _{j\in \{1,\dots ,m\}|F_{n}(x_{j})-F(x_{j})|<= \varepsilon -\frac{1}{m}$ <br>
Combined with the above result, this further implies that <br>

$ \|F_{n}-F\|_{\infty }\leq \varepsilon {\text{ a.s.}}$, which is the definition of almost sure convergence.








Ref
>chat.openai
>https://en.wikipedia.org/wiki/Glivenko%E2%80%93Cantelli_theorem

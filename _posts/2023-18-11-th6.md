---
title: Statistic th6
date: 2023-11-18 16:02:11 +/-0000
categories: [learning]
math: true
---

#  Algorithms for random variates generation

In simulation, pseudo random numbers serve as the foundation for generating samples from probability distribution models. We will now assume that the random number generator has been rigorously tested and that it produces sequences of  $ U_i$ ~ U(0,1) numbers. We now want to take the  $ U_i$ ~ U(0,1) and utilize them to generate from probability distributions.

The realized value from a probability distribution is called a random variate. Simulations use many different probability distributions as inputs. Thus, methods for generating random variates from distributions are required. Different distributions may require different algorithms due to the challenges of efficiently producing the random variables. 
herefore, we need to know how to generate samples from probability distributions.
In generating random variates the goal is to produce samples $ X_i $ from a distribution F(x) given a source of random numbers, $ U_i$ ~ U(0,1).
There are four basic strategies or methods for producing random variates:

+ Inverse transform or inverse cumulative distribution function (CDF) method
+ Convolution
+ Acceptance/Rejection
+ Mixture and Truncated Distributions


## Inverse Transform Method
The inverse transform method is the preferred method for generating random variates provided that the inverse transform of the cumulative distribution function can be easily derived or computed numerically. The key advantage for the inverse transform method is that for every $U_i$ use a corresponding  $X_i$ will be generated. That is, there is a one-to-one mapping between the pseudo-random number  $u_i$ and the generated variate  
$x_i$. <br>
The inverse transform technique utilizes the inverse of the cumulative distribution function as illustrated in Figure A.2, will illustrates simple cumulative distribution function. First, generate a number,  
$u_i$ between 0 and 1 (along the U axis), then find the corresponding $x_i$ coordinate by using  $F^{-1}$(.). <br>
For various values of $u_i$ , the $x_i$ will be properly ‘distributed’ along the x-axis. The beauty of this method is that there is a one to one mapping between  
$u_i$ and $x_i$. In other words, for each $u_i$ there is a unique $x_i$ because of the monotone property of the CDF. <br>

![image](https://github.com/Cheroberous/cheroberous.github.io/assets/102479391/b2ee91ea-9abc-44e7-bac9-e6ae7899a7e5) <br>

## Convolution
Many random variables are related to each other through some functional relationship. One of the most common relationships is the convolution relationship. The distribution of the sum of two or more random variables is called the convolution. Let $Y_i$ ∼ G(y) be independent and identically distributed random variables. Let  X= $\sum_{i=1}^{n} n_i = Y_i$. 
Then the distribution of  X is said to be the n-fold convolution of  Y. Some common random variables that are related through the convolution operation are:

+ A binomial random variable is the sum of Bernoulli random variables.
+ A negative binomial random variable is the sum of geometric random variables.
+ An Erlang random variable is the sum of exponential random variables.
+ A Normal random variable is the sum of other normal random variables.
+ A chi-squared random variable is the sum of squared normal random variables.

The basic convolution algorithm simply generates  $Y_i$ ∼G(y) and then sums the generated random variables. Let’s look at a couple of examples. By definition, a negative binomial distribution represents one of the following two random variables:

+ The number of failures in sequence of Bernoulli trials before the $r^{th}$ success, has range  {0,1,2,…}.
+ The number of trials in a sequence of Bernoulli trials until the rth success, it has range  {r,r+1,r+2,…}
 

The number of failures before the rth success, has range  {0,1,2,…}. This is the sum of geometric random variables with range  {0,1,2,…} with the same success probability. <br>
If  Y∼NB(r, p)with range {0,1,2,⋯}, then <br>
Y = $\sum_{i=1}^{r} X_i $ <br>
when  Xi∼Shifted Geometric(p) with range {1,2,3,…}, and Xi can be generated via inverse transform with: <br>

$$ 
X_i = \lfloor \frac{ln(1-U_i)}{ln(1-p)}\rfloor 
$$

## Acceptance/Rejection
In the acceptance-rejection method, the probability density function (PDF)  f(x), from which it is desired to obtain a sample is replaced by a proxy PDF,  
w(x), that can be sampled from more easily. The following illustrates how  w(x) is defined such that the selected samples from  w(x) can be used directly to represent random variates from  f(x).
The PDF  w(x) is based on the development of a majorizing function for  f(x). A majorizing function,  g(x), for  f(x), is a function such that  g(x)≥f(x)for  −∞<x<+∞. <br>
![image](https://github.com/Cheroberous/cheroberous.github.io/assets/102479391/c2d4ec65-2c52-42f9-a088-65b216492051) <br>

## Mixture Distributions, Truncated Distributions, and Shifted Random Variables
These methods allow for more flexibility in modeling the underlying randomness. The distribution of a random variable X is a mixture distribution if the CDF of  X has the form: <br>
![image](https://github.com/Cheroberous/cheroberous.github.io/assets/102479391/4e10797d-68ae-4a61-9431-eb1ac4b4a905) <br>

where 0<ωi<1, $ \sum{i=1}^{k} ω_i=1 $, k≥and $F_{X_i}$(x)is the CDF of a continuous or discrete random variable $X_i$, i=1,…,k.

Because mixture distributions combine the characteristics of two or more distributions, they provide for more flexibility in modeling. For example, many of the standard distributions that are presented in introductory probability courses, such as the normal, Weibull, lognormal, etc., have a single mode. Mixture distributions are often utilized for the modeling of data sets that have more than one mode.

An example of a mixture distributionis the **hyper-exponential distribution**. The hyper-exponential is useful in modeling situations that have a high degree of variability. The coefficient of variation is defined as the ratio of the standard deviation to the expected value for a random variable X.






Ref
>https://rossetti.github.io/RossettiArenaBook/app-rnrv-rvs.html
